In order to reimplement a set of PhysBAM methods (the set may consist of just one method
or multiple methods) as a Nimbus job, we first need to identify what variables are read
and what variables are updated by this set. We also need to determine if these variables
will be constant across all partitions, or if there will be a different copy corresponding
to each partition. For instance, velocity, particles are different for each partition.
However, scalars like dt, t and policies like boundary, advection_scalar are the same
across all partitions. Quantities that stay constant with time and are same across all
partitions can go into application data. Quantities that vary with time, but are constant
across partitions, can go into parameters (they’ll mostly be scalar quantities or small
data items). Quantities that vary for different partitions may go into data (read/ write
set) or parameters for the corresponding job.

I had broken the task advection of velocities as follows:
- Register data and job types. Right now, I have a different data type for different
partitions (6 data types for velocity advection on 2 workers). We will be changing this to
register a data with some region. Before starting a job, the worker will have to set the
region for the data. I suggest this should be done after clone and before create, since
the region information (bounding box) can be used to determine size and any other
allocation/ initialization parameters when create is called.
- Spawn jobs and set dependencies and read/ write sets. Set parameters.
- Implement data classes (data_face_arrays) that are a wrapper around Physbam data
structures, and something that Nimbus understands. These classes implement create/ clone/
serialize/ deserialize that Nimbus scheduler can call. Data also contains a region
attribute that is useful during gluing/ updating.
- I had added a get_debug_info() method to Nimbus data. This method returns a debug_id for
the class right now, which I used to check what the data in the read/ write set
corresponds to. However, I think we should be able to change this to a check on
dynamic_cast. I need this because I found relying on order in read and write sets to be
very tedious. So I encode type & region information within the data itself, and then
gather data within a job.
- Implement serialization/ deserialization for physbam data, that the Nimbus data
sub-class wraps, in proto_files.
- Write out a Param class (protocol buffer) and a class for advection specific parameters.
Param contains the region of data that a job is being spawned over. Advection parameter
contains other information like dt, t. Fields in Param are optional.
- Initialize all constant policies when application is loaded as application data.
- Within a job, if the data that the job operates on expects some pointers, but they are
not initialized correctly since we moved the data to application policies (this happens in
all jobs except advection of velocities right now), then set those pointers correctly.
- Within a job, glue all data (velocity data on different partitions) for Physbam to
operate on, using region from parameters. Then either directly call into Physbam methods,
or wrapper functions. Update write set data using update data at the end of the job. We
need to implement glue and update methods for all data except velocities (face arrays)
right now.
- Right now, my advection job is not how it should stay -- it checks if it is supposed to
operate on left region or right region and executes the corresponding code. I’ll fix this
code soon. In the final version, my advection job will not need to execute different code
for different regions (I have to remove the branch statement, rename some variables, and
remove the usage of some global variables).

We will need to restructure the wrapper methods that I have written as we implement the
remaining jobs. I feel water_driver should eventually be broken into a set of methods that
are partition independent, and a separate partition specific class. The partition specific
class will mostly contain pointers that need to be set correctly, during level-set
computation (and possibly other computation like particle computation) due to callbacks.
I am unsure about what pointers need to be set where for different methods right now.
